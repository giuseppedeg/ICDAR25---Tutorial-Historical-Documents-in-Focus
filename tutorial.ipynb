{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Recognition of Ancient Greek Characters on Papyrus\n",
    "\n",
    "In this tutorial we will develop a simple pipeline for the **detection** and **recognition** of characters written in Ancient Greek on papyrus manuscripts.  \n",
    "We will use **YOLOv8** (Ultralytics) for object detection, and we will also explore utilities to **visualize** and **export** the predictions in different formats.\n",
    "\n",
    "---\n",
    "\n",
    "## Why YOLOv8?\n",
    "\n",
    "We choose [YOLOv8 (Ultralytics)](https://github.com/ultralytics/ultralytics) because:\n",
    "- It directly supports training from COCO JSON annotation files.\n",
    "- It provides simple and clean PyTorch APIs.\n",
    "- It has excellent documentation and active support.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Workspace Overview\n",
    "\n",
    "Our workspace will contain the following components:\n",
    "\n",
    "### `data/`\n",
    "Download the dataset from [Zenodo](https://zenodo.org/records/13825619).  \n",
    "Once downloaded, extract the archive inside the `data` folder.\n",
    "\n",
    "The archive contains:\n",
    "- Training data\n",
    "- Test data\n",
    "- An `answer` folder with ground-truth annotations for the test set\n",
    "- Additional material\n",
    "\n",
    "The dataset is in **COCO format** for object detection:  \n",
    "- Images of papyri (fragments of the *Iliad*)  \n",
    "- A JSON file with bounding box annotations for every Ancient Greek character (24 classes).\n",
    "\n",
    "\n",
    "\n",
    "### `src/`\n",
    "The `src` folder contains utility code for this tutorial:\n",
    "- `coco/`: functions to compute evaluation metrics following the COCO standard  \n",
    "- `datasets/`: functions to manipulate and prepare datasets for YOLO  \n",
    "- `utils/`: additional utilities for visualization and format conversion  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèòÔ∏èCreate Environment\n",
    "Run this command to prepare your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now be careful to select your environment in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Data Preparation\n",
    "\n",
    "Download the dataset from [Zenodo](https://zenodo.org/records/13825619)  and extract yhe achive into the `data/` directory:\n",
    "\n",
    "\n",
    "The dataset is not originally split into **training** and **validation** subsets.  \n",
    "We will create our own split so that the structure looks like this:\n",
    "\n",
    "```\n",
    "dataset/\n",
    " ‚îú‚îÄ‚îÄ train/\n",
    " ‚îÇ    ‚îú‚îÄ‚îÄ images/\n",
    " ‚îÇ    ‚îî‚îÄ‚îÄ annotations.json\n",
    " ‚îú‚îÄ‚îÄ val/\n",
    " ‚îÇ    ‚îú‚îÄ‚îÄ images/\n",
    " ‚îÇ    ‚îî‚îÄ‚îÄ annotations.json\n",
    "```\n",
    "\n",
    "To do this, we can use the utilities found in `datasets.yolo_dataset`. Specifically, we can use the `create_YOLO_train_val()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.yolo_dataset import create_YOLO_train_val\n",
    "\n",
    "create_YOLO_train_val(in_dataset_path=\"data/ICDAR2023_Competition_on_Detection_and_Recognition_of_Greek_Letters_on_Papyri_Dataset/2a.Training\",\n",
    "                      out_dataset_path=\"data/YOLO_data\",\n",
    "                      bt_filter=[\"bt3\", \"bt2\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training a YOLO Model\n",
    "\n",
    "Now we can train a YOLO model to detect and recognize Ancient Greek characters.\n",
    "\n",
    "We will use the Ultralytics YOLOv8 API.\n",
    "\n",
    "**_NOTE:_**  Try different parameters for the training. In particular, consider different sizes for images: 640, 1024,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, YOLO will create a folder `checkpoints` containing the weights of the netwoek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Testing the Model\n",
    "\n",
    "We can now test the trained model on new images.\n",
    "Let‚Äôs create a folder `data/images/` and copy some test images there.\n",
    "\n",
    "**_NOTE:_** I suggest you choose from the images in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"test_icdar/yolo8_n_640/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "image_to_test = \"data/images\"\n",
    "out_folder = \"data/out\"\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated images are saved in `data/out/`.\n",
    "\n",
    "They give us a quick impression of the model‚Äôs performance, but are not always easy to edit or use further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìë Exporting Predictions\n",
    "\n",
    "For better usability, we want to export predictions in standard formats.\n",
    "\n",
    "1. COCO JSON\n",
    "\n",
    "We first export the detections into the COCO JSON format, which is supported by many third-party tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from src.datasets.yolo_dataset import converto_out_COCO\n",
    "import json\n",
    "import os \n",
    "\n",
    "image_to_test = \"data/images\"\n",
    "out_folder = \"data/out\"\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# Load the model to test\n",
    "model = YOLO(model_path) \n",
    "\n",
    "# Predict\n",
    "results = model(image_to_test)\n",
    "\n",
    "# save\n",
    "for r in results:\n",
    "    img_name = os.path.basename(r.path)\n",
    "    os.makedirs(os.path.join(out_folder, os.path.splitext(img_name)[0]), exist_ok=True)\n",
    "\n",
    "    # save JSON\n",
    "    coco_data = converto_out_COCO(r)\n",
    "    with open(os.path.join(out_folder, os.path.splitext(img_name)[0], \"prediction.json\"), \"w\") as f:\n",
    "        json.dump(coco_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save output\n",
    "    r.save(os.path.join(out_folder, os.path.splitext(img_name)[0], \"prediction.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format requires the original images to visualize annotations properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PAP Format\n",
    "\n",
    "To simplify distribution and interaction, we can encapsulate both the image and the annotations in a single `.PAP` file.\n",
    "This way, users only need to handle one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.target_utils import create_pap\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "out_dir = \"data/out\"\n",
    "img_dir = \"data/images\"\n",
    "\n",
    "for img_name in tqdm(os.listdir(img_dir), desc=\"Create PAP files\"):\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    json_file = os.path.join(out_dir, os.path.splitext(img_name)[0], \"prediction.json\")    \n",
    "\n",
    "    create_pap(img_path, json_file, save_img=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `.PAP` files can be shared and visualized independently.\n",
    "\n",
    "For example, using the online free tool [GlyFix](https://glyfix.scicore.unibas.ch/corrector.html?toml=pyscript.toml&project_file=new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "In this final section, we evaluate the performance of different YOLO models trained on the Iliad papyrus dataset.  \n",
    "The comparison is based on the **COCO-style mean Average Precision (mAP)**, using the standard metric:\n",
    "\n",
    "- **mAP@[.5:.95]**: Average precision across IoU thresholds from 0.5 to 0.95 (step 0.05).  \n",
    "\n",
    "We calculate the separate performances for Character Detection only and then for the Recognition phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import json\n",
    "from src.datasets.yolo_dataset import converto_out_COCO\n",
    "from src.datasets.categories import CATEGORIES_DECODING\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from src.coco.coco_eval import summarizeCustom\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "model = YOLO(model_path) \n",
    "\n",
    "\n",
    "testset_path = \"data/ICDAR2023_Competition_on_Detection_and_Recognition_of_Greek_Letters_on_Papyri_Dataset/2b.Testing\"\n",
    "test_path = \"data/ICDAR_test.json\"\n",
    "\n",
    "COCOeval.summarizeCustom = summarizeCustom\n",
    "\n",
    "# create COCO gt\n",
    "if not os.path.exists(os.path.join(testset_path, \"test.json\")):\n",
    "    with open(test_path) as f:\n",
    "        coco_data_gt = json.load(f)\n",
    "    img_ids = []\n",
    "    for img in coco_data_gt[\"images\"]:\n",
    "        img_ids.append(img[\"id\"])\n",
    "    #img_ids = [3]\n",
    "    images = []\n",
    "    for img in coco_data_gt[\"images\"]:\n",
    "        if img[\"id\"] in img_ids:\n",
    "            images.append(img)\n",
    "\n",
    "    annotations = []\n",
    "    for ann in coco_data_gt['annotations']:\n",
    "        if ann[\"image_id\"] in img_ids:\n",
    "            annotations.append({\n",
    "                \"id\": ann['id'],\n",
    "                \"image_id\": ann['image_id'],\n",
    "                \"category_id\": ann['category_id'],\n",
    "                \"bbox\": ann['bbox'],\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": ann['bbox'][2] * ann['bbox'][3]\n",
    "            })\n",
    "    coco_data_gt['annotations'] = annotations\n",
    "    coco_data_gt['images'] = images\n",
    "    coco_data_gt['info'] = {\"version\": 0}\n",
    "    with open(os.path.join(testset_path, \"test.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(coco_data_gt, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "coco_gt = COCO(os.path.join(testset_path, \"test.json\"))\n",
    "\n",
    "# predict images\n",
    "image_ids = coco_gt.getImgIds()\n",
    "#image_ids = [3]\n",
    "coco_results = []\n",
    "\n",
    "ann_id = 0\n",
    "for img_id in image_ids:\n",
    "    img_info = coco_gt.loadImgs(img_id)[0]\n",
    "    file_name = img_info[\"img_url\"].replace(\"./\", \"\").replace(\"homer2/\", \"\")\n",
    "    img_path = os.path.join(testset_path, file_name)\n",
    "\n",
    "    results = model(img_path)\n",
    "\n",
    "    coco_data = converto_out_COCO(results[0], img_id=img_id, ann_id=ann_id)\n",
    "    ann_id += len(coco_data['annotations'])\n",
    "\n",
    "    for ann in coco_data['annotations']:\n",
    "        ann['category_id'] = CATEGORIES_DECODING[ann['category_id']]\n",
    "        coco_results.append(ann)\n",
    "\n",
    "with open(\"predictions_test.json\", \"w\") as f:\n",
    "    json.dump(coco_results, f, indent=4)\n",
    "\n",
    "coco_dt = coco_gt.loadRes(\"predictions_test.json\") # pass only the annotations\n",
    "\n",
    "os.remove('predictions_test.json')\n",
    "\n",
    "# Compute performance\n",
    "cocoEval = COCOeval(coco_gt,coco_dt,'bbox')\n",
    "cocoEval.params.maxDets = [10000]\n",
    "# cocoEval.params.iouThrs = [0.1, 0.9]    \n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarizeCustom(verbose=True)\n",
    "labelscore = cocoEval.stats[0]\n",
    "\n",
    "cocoEval.params.useCats = False\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarizeCustom(verbose=True)\n",
    "nolabelscore = cocoEval.stats[0]\n",
    "\n",
    "print(\"Score (no labels) : \" + str(nolabelscore))\n",
    "print(\"Score (labels) : \" + str(labelscore))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We report the results for different training configurations:\n",
    "\n",
    "|               _Model_                | _mAP Detection_ | _mAP Recognition_ |\n",
    "|:-------------------------------------|:---------------:|:-----------------:|\n",
    "|  YOLOv8n img_size:640  [bt1 bt2 bt3] |      0.059      |       0.010       |\n",
    "|  YOLOv8n img_size:640  [bt1 bt2]     |      0.056      |       0.012       |\n",
    "|  YOLOv8n img_size:640  [bt1]         |      0.047      |       0.012       |\n",
    "|  YOLOv8n img_size:1024 [bt1 bt2 bt3] |      0.159      |       0.045       |\n",
    "|  YOLOv8n img_size:1024 [bt1 bt2]     |      0.156      |       0.059       |\n",
    "|  YOLOv8n img_size:1024 [bt1]         |      0.133      |       0.055       |\n",
    "|  YOLOv8n img_size:1280 [bt1 bt2 bt3] |      0.195      |       0.064       |\n",
    "|  YOLOv8n img_size:1280 [bt1 bt2]     |      0.200      |       0.087       |\n",
    "|  YOLOv8n img_size:1280 [bt1]         |      0.133      |       0.055       |\n",
    "|                                      |                 |                   |\n",
    "|  YOLOv8s img_size:640  [bt1 bt2 bt3] |      0.150      |       0.075       |\n",
    "|  YOLOv8s img_size:640  [bt1 bt2]     |      0.158      |       0.082       |\n",
    "|  YOLOv8s img_size:640  [bt1]         |      0.046      |       0.012       |\n",
    "|  YOLOv8s img_size:1024 [bt1 bt2 bt3] |      0.276      |       0.164       |\n",
    "|  YOLOv8s img_size:1024 [bt1 bt2]     |      0.265      |       0.167       |\n",
    "|  YOLOv8s img_size:1024 [bt1]         |      0.216      |       0.147       |\n",
    "|  YOLOv8s img_size:1280 [bt1 bt2 bt3] |    **0.307**    |       0.197       |\n",
    "|  YOLOv8s img_size:1280 [bt1 bt2]     |      0.289      |     **0.199**     |\n",
    "|  YOLOv8s img_size:1280 [bt1]         |      0.321      |       0.167       |\n",
    "\n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Smaller models (e.g. **YOLOv8n**) are much faster and lighter, but their detection accuracy is limited.  \n",
    "- Larger models (e.g. **YOLOv8l**) provide higher mAP values, but require more computation and memory.  \n",
    "- For correct character detection, images must be large enough to make the characters identifiable and recognizable.\n",
    "- Large images, however, make the training process slower and require more GPU memory.\n",
    "- Training with poorly preserved characters (bt3) does not have a very noticeable effect on the network's detection performance, but it increases the recognition a bit.\n",
    "- For the purpose of recognizing Ancient Greek characters on papyri, the **trade-off between accuracy and efficiency** must be carefully considered depending on the application (e.g., large-scale automatic transcription vs. interactive annotation support).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Summary\n",
    "\n",
    " - We trained a YOLOv8 model for detecting 24 Ancient Greek characters on papyrus images.\n",
    "\n",
    " - We prepared the dataset (split into train/val), trained the model, and tested it.\n",
    "\n",
    " - We exported predictions in COCO JSON and PAP formats.\n",
    "\n",
    " - Using external tools such as GlyFix, scholars can correct and refine annotations.\n",
    "\n",
    "This workflow shows how modern AI methods can support digital paleography."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icdar_tutorial_hist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
